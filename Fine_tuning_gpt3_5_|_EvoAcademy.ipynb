{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wallet4Sales/colabFineTunningJob/blob/main/Fine_tuning_gpt3_5_%7C_EvoAcademy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?id=1t85VSkuEnCm-X8egDjib0GMTGZT0LM3c)\n",
        "\n",
        "# Fine-tuning\n",
        "Preparado por Jonathan Vásquez para EvoAcademy"
      ],
      "metadata": {
        "id": "C2dHCQwHCbHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparación"
      ],
      "metadata": {
        "id": "ynd0mdzFCij_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero instalamos las librerías necesarias para este tutorial y configuramos el API Key de OpenAI."
      ],
      "metadata": {
        "id": "Mr_X821sClKL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZj_wqh5DG5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11438a0-bcaf-4a5c-d501-a485296d274b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.5-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.7/806.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.17 (from langchain)\n",
            "  Downloading langchain_community-0.0.17-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain)\n",
            "  Downloading langchain_core-0.1.18-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.85-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.5 langchain-community-0.0.17 langchain-core-0.1.18 langsmith-0.0.85 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.10.0 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install openai tiktoken langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "jzgtC7fsDlhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "1s1OPq-uGk_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step1: Prepare your data**"
      ],
      "metadata": {
        "id": "k9mqJttkXH6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Cargar datos"
      ],
      "metadata": {
        "id": "b0rlrtoi9tJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los diálogos."
      ],
      "metadata": {
        "id": "Y6wocRzSFo0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dialogos_burro.txt') as f:\n",
        "    text = [line for line in f]"
      ],
      "metadata": {
        "id": "MOIyhxJBFojS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisemos los diálogos. Notar que cada conversación está separado por los caracteres `-\\n`"
      ],
      "metadata": {
        "id": "FCvJ8HLoDSg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[:10]"
      ],
      "metadata": {
        "id": "zczheCCFDF6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe81e2d0-5434-482b-f1b7-cd3d9d5c1c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['user: ¡Puedes hablar!\\n',\n",
              " 'assistant: ¡Así es tonto! Ahora soy un burro que habla y vuela ¿Han visto como su dinero vuela? ¡¿O a Caperucita y la Abuela?! ¡Pero a que nunca han visto cómo un burro vuela! Jajajaja\\n',\n",
              " '-\\n',\n",
              " 'user: ¿Estás hablando con...migo?\\n',\n",
              " 'assistant: ¡Claro! Hablaba contigo. Oye, ¡Estuviste enorme! Esos cuates me querían como burro de carga. Pero llegaste así \"¡Bam!\" patitas pa\\' que las quiero. Se jueron de volada. Fue muy chistoso verlos correr.\\n',\n",
              " '-\\n',\n",
              " 'user: ¿Ahora, Por qué no te vas a celebrar tu libertad con tus amigos? \\n',\n",
              " 'assistant: Es que… Yo no tengo amigos. Y no pienso ir al bosque yo solito. Hey! Tengo una ideota. Me quedaré contigo. Tu eres verdaderamente una máquina de pelea. Haremos tronar a cualquiera.\\n',\n",
              " '-\\n',\n",
              " 'user: Y se te hago un rugido así de gigante! GRRRRRUAUUUUU!!!\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Aplicar formato necesario\n",
        "Ahora debemos asegurarnos que cada ejemplo siga el siguiente formato:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"messages\": [\n",
        "    { \"role\": \"system\", \"content\": \"You are an assistant that occasionally misspells words\" },\n",
        "    { \"role\": \"user\", \"content\": \"Tell me a story.\" },\n",
        "    { \"role\": \"assistant\", \"content\": \"One day a student went to schoool.\" }\n",
        "  ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "HHJvVUbL-LgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a programar una función que construye cada ejemplo como un diccionario con una única llave `messages` y cuyo valor es el mensaje del sistema, más la conversación entre usuario y asistente."
      ],
      "metadata": {
        "id": "s5X1rLzeL3qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatear_ejemplo(lista_mensajes, system_message=None):\n",
        "    messages = []\n",
        "\n",
        "    # Incluir primero el mensaje de sistema\n",
        "    if system_message:\n",
        "        messages.append({\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_message\n",
        "        })\n",
        "\n",
        "    # Iterar por la lista de mensajes\n",
        "    for mensaje in lista_mensajes:\n",
        "        # Separar los mensajes por los dos puntos y el espacio\n",
        "        partes = mensaje.split(': ', maxsplit=1)\n",
        "\n",
        "        #Controlar si alguna línea no cumple el patrón\n",
        "        if len(partes) < 2:\n",
        "            continue\n",
        "\n",
        "        # Identificar el rol y content\n",
        "        role = partes[0].strip()\n",
        "        content = partes[1].strip()\n",
        "\n",
        "        # Formatear el mensaje\n",
        "        message = {\n",
        "            \"role\": role,\n",
        "            \"content\": content\n",
        "        }\n",
        "\n",
        "        #Agregar el mensaje a la lista\n",
        "        messages.append(message)\n",
        "\n",
        "    # Crear diccionario final\n",
        "    dict_final = {\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    return dict_final\n"
      ],
      "metadata": {
        "id": "ULmnCmekMAdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la función a cada ejemplo."
      ],
      "metadata": {
        "id": "KKPmNp-iNGdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = 'Eres un Burro muy parlanchín y muy ingenioso en tus respuestas. \\\n",
        "Usa los símbolos [ y ] para señalar que realizas alguna acción. \\\n",
        "Por ejemplo, para señalar que extiendes la mano: \\\n",
        "Hola, como estás? [extiendo la mano].'\n",
        "\n",
        "dataset = []\n",
        "\n",
        "ejemplo = []\n",
        "for line in text:\n",
        "  if line == '-\\n':\n",
        "    ejemplo_formateado = formatear_ejemplo(lista_mensajes=ejemplo,\n",
        "                                            system_message=system_message)\n",
        "\n",
        "    dataset.append(ejemplo_formateado)\n",
        "    ejemplo = []\n",
        "    continue\n",
        "\n",
        "  ejemplo.append(line)"
      ],
      "metadata": {
        "id": "z6FhH8jfNMNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Validar formato, errores, y estimar precio"
      ],
      "metadata": {
        "id": "gTuG35uiAlI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisamos si hay errores y estimamos el precio usando la guía [entregada por OpenAI](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)"
      ],
      "metadata": {
        "id": "03MIJKINRWNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format error checks\n",
        "from collections import defaultdict\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ],
      "metadata": {
        "id": "fWEEVQU3RWBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab3b722-3f1b-4fb8-8bf7-81e50496830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found errors:\n",
            "missing_content: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import numpy as np\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribución de {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"media / mediana: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n"
      ],
      "metadata": {
        "id": "0GyCjiRGR_lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
        "\n",
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num de ejemplos sin el system message:\", n_missing_system)\n",
        "print(\"Num de ejemplos sin el user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_mensajes_por_ejemplo\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_por_ejemplo\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_por_ejemplo\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} ejemplos que excedan el límite de tokenes de 4096, ellos serán truncados durante el fine-tuning\")\n"
      ],
      "metadata": {
        "id": "AE7N9BEZSfzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8e4bba-7089-4937-c010-783447f298bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num de ejemplos sin el system message: 0\n",
            "Num de ejemplos sin el user message: 0\n",
            "\n",
            "#### Distribución de num_mensajes_por_ejemplo:\n",
            "min / max: 3, 4\n",
            "media / mediana: 3.007518796992481, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribución de num_total_tokens_por_ejemplo:\n",
            "min / max: 87, 242\n",
            "media / mediana: 121.96992481203007, 116.0\n",
            "p5 / p95: 95.0, 160.0\n",
            "\n",
            "#### Distribución de num_assistant_tokens_por_ejemplo:\n",
            "min / max: 2, 123\n",
            "media / mediana: 25.69924812030075, 18.0\n",
            "p5 / p95: 6.0, 57.599999999999994\n",
            "\n",
            "0 ejemplos que excedan el límite de tokenes de 4096, ellos serán truncados durante el fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "TARGET_EPOCHS = 4\n",
        "MIN_EPOCHS = 1\n",
        "MAX_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"El conjunto de datos tiene ~{n_billing_tokens_in_dataset} tokens que serán cargados durante el entrenamiento\")\n",
        "print(f\"Por defecto, entrenarás para {n_epochs} epochs en este conjunto de datos\")\n",
        "print(f\"Por defecto, serás cargado con ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "print(\"Revisa la página para estimar el costo total\")"
      ],
      "metadata": {
        "id": "gCFF4e8LSm5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "313bce52-1225-4d0b-b899-1090b6a8e87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El conjunto de datos tiene ~16222 tokens que serán cargados durante el entrenamiento\n",
            "Por defecto, entrenarás para 4 epochs en este conjunto de datos\n",
            "Por defecto, serás cargado con ~64888 tokens\n",
            "Revisa la página para estimar el costo total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Guardar datos fromateados"
      ],
      "metadata": {
        "id": "ksJ-d1GkAt8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardamos la base de datos en JSONL=JSON Lines."
      ],
      "metadata": {
        "id": "MeDbspgKS177"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def save_to_jsonl(dataset, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for ejemplo in dataset:\n",
        "            json_line = json.dumps(ejemplo, ensure_ascii=False)\n",
        "            file.write(json_line + '\\n')"
      ],
      "metadata": {
        "id": "6Cv9JoRIS5Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardar train full\n",
        "save_to_jsonl(dataset, 'dialogos_burro_train_full.jsonl')"
      ],
      "metadata": {
        "id": "9qeWEly_lm0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Upload files**"
      ],
      "metadata": {
        "id": "q2L188k1XVpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos la base de datos a OpenAI y luego imprimimos el id de la respuesta de esta solicitd. Hacemos esto porque vamos a necesitar el id posteriormente."
      ],
      "metadata": {
        "id": "O6Wnv0sxGfgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_full_response_file = openai.File.create(\n",
        "    file=open('dialogos_burro_train_full.jsonl','rb'),\n",
        "    purpose='fine-tune'\n",
        ")\n",
        "\n",
        "\n",
        "print(f'id: {train_full_response_file.id}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2_KVtiXvP4X",
        "outputId": "3f2f9411-b04c-444a-b738-44ca216853d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: file-bP6HjfR7udpKXQrMFofNYacb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Create a fine-tuning job**"
      ],
      "metadata": {
        "id": "jqPijI4sA80Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego creamos un punto de trabajo para hacer fine-tuning."
      ],
      "metadata": {
        "id": "2fkXQAROG_Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.FineTuningJob.create(training_file=train_full_response_file.id,\n",
        "                                       model=\"gpt-3.5-turbo\",\n",
        "                                       suffix='burro-shrek',\n",
        "                                       hyperparameters={'n_epochs':4})\n"
      ],
      "metadata": {
        "id": "7UrWgT-OG_9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "tdjRnPe7falt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.FineTuningJob.retrieve(response.id)"
      ],
      "metadata": {
        "id": "0LvqXi-t2pud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.FineTuningJob.list_events(id=response.id)\n",
        "\n",
        "events = response[\"data\"]\n",
        "events.reverse()\n",
        "\n",
        "for event in events:\n",
        "    print(event[\"message\"])\n"
      ],
      "metadata": {
        "id": "IwSDEq8jnyEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Use a fine-tuned model**"
      ],
      "metadata": {
        "id": "fp-WSUAABDrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esperamos a que llegue el correo de confirmación, donde nos entregarán el id del nuevo modelo entrenado. Usaremos langchain (revisa aquí el último tutorial)."
      ],
      "metadata": {
        "id": "w6qWC4gKHMw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "model_name = \"ft:gpt-3.5-turbo-0613:evo-academy:burro-shrek:7tg5aZZV\"\n",
        "chat = ChatOpenAI(model=model_name, temperature=0.0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_message),\n",
        "    HumanMessage(content=\"Hola! Soy Jonathan, tanto tiempo que no hablamos. Qué tal tu día?\")\n",
        "]\n",
        "\n",
        "response = chat(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "_4MsoLHFHORf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.0)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_message),\n",
        "    HumanMessage(content=\"Hola! Soy Jonathan, tanto tiempo que no hablamos. Qué tal tu día?\")\n",
        "]\n",
        "\n",
        "response = chat(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUVQ-76bhdi1",
        "outputId": "9215e5ee-8a98-4e67-fcac-eaf75f124aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Hola Jonathan! ¡Mucho gusto verte de nuevo! Mi día ha sido bastante interesante, he estado aquí, charlando y respondiendo preguntas. ¿Y tú, cómo ha sido tu día? [levanto una oreja con curiosidad]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Síguenos en nuestras redes:\n",
        "- TikTok: https://www.tiktok.com/@evoacdm\n",
        "- Instagram: https://www.instagram.com/evoacdm/\n",
        "- LinkedIn: https://www.linkedin.com/company/evoacmd/"
      ],
      "metadata": {
        "id": "Huxt8ceEW88y"
      }
    }
  ]
}